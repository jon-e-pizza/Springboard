{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling for Our Disaster Tweets\n",
    "\n",
    "As the input training data for this project is coming out of a Kaggle competition, it is already gathered in it's entirety (and fairly well wrangled). Instead of gathering and combination, our focus will be on processing our data with machine learning techniques to accurately predict the concern that a tweet should merit. To prepare for this process, while our input tweet training set is as complete as it will be coming in, the models we will use will benefit from additional features that can be derived from the data:\n",
    "\n",
    "1. Processed Tweet texts\n",
    " - with tokens with low predictive value such as stop words or punctuation removed\n",
    " - with remaining tokens lemmatized for increased token predictive value across the corpus\n",
    " - with common bigrams ajoined for increased predictive value\n",
    "2. Primary likely topic of the tweet \n",
    "3. Representation of tweets in TF-IDF vector form\n",
    "\n",
    "The first of these two steps will be appended to our original data set to produce an ammended CSV, the third step set of features will be stored in a seperate CSV to maintain readability of the first file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Consistency\n",
    "\n",
    "Before performing our feature engineering steps, we should first do a sanity check on our incoming data to make sure we have the inputs we expect. First, all tweets should have an integer 0 or 1 in their 'target' column indicating whether the tweet is a disaster or not, and second, all tweet samples should have a string object in their text column. If any sample lacks either of these features they should be removed from our data set before we begin our feature engineering.\n",
    "\n",
    "Some subset of tweets also include keyword and location, but as they are not required, we will not be filtering our samples based on these fields. We may use keywords as a predictive variable in our machine learning step later, while we will pass over the location variable as not being relevant to our problem of automating answering whether the text of a tweet indicates it deserves attention from human emergency response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweet_df = pd.read_csv('../data/kaggle_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# check if we have nulls in our columns\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of the text entries are empty\n",
      "0 tweets aren't correctly targeted\n"
     ]
    }
   ],
   "source": [
    "# no nulls, but lets verify fields aren't otherwise invalid\n",
    "print(\"{} of the text entries are empty\".format(\n",
    "    len(tweet_df[tweet_df.text.str.len() == 0])))\n",
    "print(\"{} tweets aren't correctly targeted\".format(\n",
    "    len(tweet_df[~((tweet_df.target == 0) | (tweet_df.target == 1))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Tweet Processing\n",
    "\n",
    "Our fields are consistent, so it's time to start engineering our features. The first thing we aim to do is use our preprocessing toolkits to provide some normalization over our tweet texts. We'll be normalizing via stripping out white space, so called stop words ('the', 'a', etc.), and punctuation with little predictive value. We'll lemmatize remaining words after these filters to increase the amount of predictive information we can get from common terms, and then use phrase modeling to ajoin potential bigrams in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We'll use Spacy for our tweet preprocessing, and add emoticons to our\n",
    "# pipeline so we don't remove them as simple punctuation tokens\n",
    "import spacy\n",
    "from spacyemoticon import Emoticon\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "emoticon = Emoticon(nlp)\n",
    "nlp.add_pipe(emoticon, first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And while stop words are likely to not be good signal for our final models, we're also bound to find some tokens in our tweets that lack in predictive value that aren't in the spacy dictionary at all, ie. 'http'. We'll traverse our tweets for words outside the spacey vocabulary that we think do have predictive value (ie emoticons), and let the rest be filtered out in our final processed tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first some utility processing\n",
    "def usable_token(tok):\n",
    "    \"\"\" return if token could have predictive value \"\"\"\n",
    "    return not (tok.lemma_ in spacy.lang.en.stop_words.STOP_WORDS\n",
    "                or tok.lemma_ == 'rt' # added after discovery in exploration\n",
    "                or tok.is_space\n",
    "                or (tok.is_punct and not tok._.is_emoticon))\n",
    "\n",
    "def left_hash(tok):\n",
    "    \"\"\"\n",
    "    Return hashtag if token is a hashtag\n",
    "\n",
    "    Words are used with slightly different emphasis when used as\n",
    "    hashtags, so we'll maintain this distinction post processing\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tok : nlp.Token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        Either a '#' character to be prepended to a recognized hashtag\n",
    "        token or empty string '' if not a hashtag\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if tok.nbor(-1).orth_ == '#':\n",
    "            return '#'\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a set of oov tokens\n",
    "oov_tokens = set()\n",
    "\n",
    "def is_oov(tok):\n",
    "    if not usable_token(tok) or left_hash(tok) == '#':\n",
    "        # we want to keep our emoticons and hashtags\n",
    "        return False\n",
    "    else:\n",
    "        return tok.is_oov\n",
    "\n",
    "def tt_oovs(nlp, tt):\n",
    "    parsed = nlp(tt)\n",
    "    for token in parsed:\n",
    "        if is_oov(token):\n",
    "            oov_tokens.add(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need a decorator to apply to our dataframe\n",
    "def df_nlp_app(nlp, func):\n",
    "    return lambda text: func(nlp, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "7608    None\n",
       "7609    None\n",
       "7610    None\n",
       "7611    None\n",
       "7612    None\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.text.apply(df_nlp_app(nlp, tt_oovs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see what we caught:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8920\n"
     ]
    }
   ],
   "source": [
    "print(len(oov_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is too many to sweep at once, lets see if we can summarily shorten that list in anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://t.co/vcq2icptki', '-population:6', 'http://t.co/w7siidujoh', 'http://t.co/irqujaesck', 'http://t.co/f9j3l2yjl4', '@rejectdcartoon', 'http://t.co/i1vpkq9yag', 'http://t.co/rqu5ub8plf', 'http://t.co/gyzpisbi1u', 'http://t.co/kgkz50q8tk', 'http://t.co/ns5lbs5zup', 'https://t.co/ma4ra7atql', 'http://t.co/qwijrriyif', 'http://t.co/nlfr8t3xqm', 'oamsgajagahahah', 'http://t.co/nnylxhinpx', 'http://t.co/weudlkc4o4', '40hourfamine', 'http://t.co/fj73gdvg2n', 'http://t.co/8rdxcfgqem', 'http://t.co/up30aqgnlf', '@slatukip', 'https://t.co/yrfz5wj7r2', 'http://t.co/g5zsru0zvq', 'http://t.co/pbya7uv3v5', 'http://t.co/wnptvbm5t7', 'matako_3', 'http://t.co/bnhtxaezmm', 'http://t.co/xfhh2xf9ga', 'pic.twitter.com/pnpizody', 'http://t.co/tgdonttkty', 'favori', '@davidvonderhaar', 'http://t.co/tyyfg4qqvm', '@rzimmermanjr', 'http://t.co/ykuauov9jo', 'http://t.co/xvco7slxhw', 'http://t.co/qgum9xheos', 'http://t.co/2zgiupn06t', 'http://t.co/qvx0vqtpz0', 'http://t.co/ae9cpiexak', 'harda', 'http://t.co/qna4tubnwh', '@argus_99', 'http://t.co/lma39zrwoy', 'http://t.co/h6ngsw9a5b', 'intro+desolation', 'http://t.co/1k2phqcuw8', '@katiecool447', '@slikrickdarula']\n"
     ]
    }
   ],
   "source": [
    "print(list(oov_tokens)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually tells us that we'll have to do a bit more processing on some of our lemmas to remove '\\x89û', but also tells us that we can probably start minimizing our group by removing all mentions and addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_oov = set()\n",
    "for token in oov_tokens:\n",
    "    if not (token.startswith(\"http\") or token.startswith('@') or token.endswith(\"\\x89û\")):\n",
    "        filtered_oov.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sk398', '-population:6', 'rabaa', 'wild#fire', '\\x89û÷hoax', 'mitt.\\x89û\\x9d', '~peace', 'totoooooooooo', '1600-year', 'zones***thank', '\\x89û÷the', 'reaad/', '3-inspired', 'pugwash', 'oamsgajagahahah', '40hourfamine', 'full\\x89ã¢', 'macabrelolita', 'ks57', '429cj', 'matako_3', 'lilourry', 'pic.twitter.com/pnpizody', 'favori', 'goodlook', '-=-0!!!!.', 'llegaste', 'joeysterling', 'harda', 'i77', 'intro+desolation', 'lastingness', 'timestack', '2.4regionåêåênear', '\\x89û÷05', 'glononium', 'airplaneåê(29', 'intact+mh370+part+lifts+odds+plane+glided+not+crashed+into+sea', 'mumbai24x7', 'australia\\x89ûªs', 'socialwots', 'jimin', 'arm\\x89ûò', 'since1970the', 'servicin', '9:12pm', 'vibez', 'entertainer\\x89û\\x9d', 'laighign', 'fwt']\n"
     ]
    }
   ],
   "source": [
    "print(list(filtered_oov)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting closer, but we see more forms of '\\x89' that we'll want to filter out in our final tweets. At the moment lets just shrink our set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_oov2 = set()\n",
    "for token in filtered_oov:\n",
    "    if not (token.endswith(\"\\x89ûª\") or token.endswith(\"\\x89ûò\")):\n",
    "        filtered_oov2.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1854\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_oov2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had hardly any gain from this filter, so at this point we may need to just see if we can catch any tokens to save by a manual sweep over the remaining tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sk398', '-population:6', 'rabaa', 'wild#fire', '\\x89û÷hoax', 'mitt.\\x89û\\x9d', '~peace', 'totoooooooooo', '1600-year', 'zones***thank', '\\x89û÷the', 'reaad/', '3-inspired', 'pugwash', 'oamsgajagahahah', '40hourfamine', 'full\\x89ã¢', 'macabrelolita', 'ks57', '429cj', 'matako_3', 'lilourry', 'pic.twitter.com/pnpizody', 'favori', 'goodlook', '-=-0!!!!.', 'llegaste', 'joeysterling', 'harda', 'i77', 'intro+desolation', 'lastingness', 'timestack', '2.4regionåêåênear', '\\x89û÷05', 'glononium', 'airplaneåê(29', 'intact+mh370+part+lifts+odds+plane+glided+not+crashed+into+sea', 'mumbai24x7', 'australia\\x89ûªs', 'socialwots', 'jimin', 'since1970the', 'servicin', '9:12pm', 'vibez', 'entertainer\\x89û\\x9d', 'laighign', 'fwt', 'efs300', '1!(preview', 'ferno', 'stormcoming', 'lisowski', 'referencereference', 'doing\\x89û_ahhh\\x89û_that', 'hua', 'matako_milk', '3_6_12', 'udom', 'yennora', '\\x89û÷extremely', 'themhe', 'hillarymass', 'womengirls', 'vancouveråêisland', 'caaaaaall', 'orchs', ':)))', 'as10004', 'megalpolis', 'skyåênews', 'en}tension', 'stlnd', 'gt;&gt', 'psm', '7:12am', 'sct014', 'flatback', 'mopheme', 'friend50', '):', 'bridgework', 'awesomelove', 'mark', \"onåê'the\", 'palinfoen', 'dajaal', 'pusssssssssy', 'barcousky', 'recip', 'bhaijaan', '1pack', 'c-5040-h', 'm151a1', 'ads.it', 'drumpf', 'jhmnye', 'hah-', 'ibrahimmisau', 'splatling', 'raung', 'goooooooaaaaaal', 'ameribag', '\\x89ûïdetonate\\x89û\\x9d', 'undermine]d', 'specialguest', '.@jimmyfallon', 'kulli', '2500fps', 'hwrf', \"son'd\", 'indiahttp://www.informationng.com/?p=309943', 'alsowhat', 'yeda', 'å£6bn', 'boltåêcyclone', 'nbc10', 'twit_san_diego', '\\x89ûòåêcnbc', 'miì', 'grant_factory', 'dredougie', 'njenga', 'gt;50', 'crash&gt;http://t.co', 'gbonyin', 'wnowfvcbm', 'thesensualeye', '1-russian', 'soudelor', 'way.\\x89û\\x9dyeah', '#fire', 'thankkk', 'carmel', 'mansehra', 'kororinpa', '18jst', '\\x89ûïfor', '\\x89û÷good', 'with---', 'genisy', 'man\\x89ûªs', 'here\\x89ûªs', '01:04:01', 'home(s)/sponsorship', '01:50:25', 'sothwest', 'livemint', 'pierc', '29/20', 'mizuta', 'teamo', 'wedn', 'wxia', 'slsp', 'snotgreen', '\\x89û÷politic', 'w--=-=-=-', 'racco', 'pull\\x89û_one\\x89û_you', '90225', 'electr', 'a---', 'aseer', '^ag', '-sister', 'contig', 'zimpapersviews', '3-inspir', 'standard.#anonymous', 'arsonist', 'clergyforc', 'redeemeth', 'san\\x89ûªa', 'naemolgo', 'number', 'elecman', 'day.http://t.co/8vzl1ns2io', 'nikoniko12022', 'reveillertm', 'dying!they', '\\x89û¢\\x89û¢if', 'ks94', \"ugly'@amesocialaction\", '10news', 'qave', '55436', 'twx', '\\x89ûïlittle', 'superintende', 'x1434', 'korzhonov', '\\x89û÷bomb', '20177', '\\x89û÷we', 'ì¢', 'pikin', 'sct012', 'pakpattan', 'qendil', '/shakespeare', 'time', 'cope-', 'flechadas', 'ykelquiban', '10:38pm', '16:40:21', 'hey!sundown', 'arnley', '\\x89û÷hijacker', 'bayelsa', \"7'46\", 'roskomnadzor', '9:31pm', 'nataly', 'newåêheights', 'shaabi', 'temporary:300', 'lpdkl', 'indot', 'invzice', 'ombudsmanship', 'queenåê', '4678', 'argsuppose', '96355', '-04:00', 'bioterrorism&amp;use', \"tolewant'g\", 'm6x1.00', 'bilsen', 'killsåêone', '6:30/8:30', 'individl', 'pileq', 'laughtrader', 'can\\x89ûªt', 'juanmthompson', '.@colts', 'pagasa', 'kijangbank', 'roga', 'ashayo', 'ploppy', '.@dinosaurdracula', '11:03:58', 'tawfmcaw', 'mumbaitime', 'keithyy', 'vì_deo', 'ln;col;yellowstone', 'silinski', 'influencers', 'sb57', 'i-540', 'i580', 'selmo', 'iredell', 'mydrought', 'catlow', 'fuso', 'starbs', 'sunnymeade', 'åèmgn', 'uud+lk', 'ktfounder', '-honey', 'lt;meltdown', 'comeeeee', 'f496d', '\\x89ûïmake', '--thus', 'bitch!*run', 'membuahkan', 'wakho', 'russaky89', 'bokuto', 'eovm', 'diretube', 'flatliner', '9:33pm', 'everwhe', '\\x89ûòthe', 'collision-1141', '00:11:16', 'hough_jeff', 'å¤', 'nflexpertpick', 'time\\x89û\\x9d', 'sandune', 'ks161', 'qnh', 'top25', 'trixie_drown', '10:34:24', 'lassics', 'liguistic', 'mudslide', 'leicester_merc', 'die!please', 'disaster\\x89û¢', 'gt;http://t.co', 'songfor', 'pres.bad', 'tps=', '\\x89ûï@ymcglaun', 'warra', '4kus', '1/13/82', 'wsvr1686b', 'bordento', '-pandemonium', '\\x89û¢', 'oktaviana', '4608ìñ2474', 'me\\x89û_turn', \"talk='ecology'&amp;'human\", '\\x89ûïcat', 'modibo', 'enrt', 'khulna', 'looks', '//im', '\\x89ûïpartie', \"bioterrorismi'm\", 'b4federal', '16:11:16', 'obta', 'high\\x89û\\x9d', 'preston\\x89ûªs', '0880', 'hyider_ghost2', \"43'c\", 'seasonfrom', 'night?sad', '\\x89ûï@macdaddy_leo', 'icemoon', 'ercjmnea', 'onlinemh370', 'dfljev', 'waimate', 'naved', 'twia', '\\x89ûï#hannaph\\x89û\\x9d', 'imeshika', 'deglin', 'championsblackfoot', '.@stavernise', 'news3lv', 'thrarchives', '.@mwlippert', 'biosurveillance', 'offensiveåêcontent', 'wqow', 'amp;want', 'hawaiianpaddlesport', '12news', 'sequalae', 'muzzamil', 'gmtty', 'billneelynbc', 'invokce', 'master0fsloth', '.@uriminzok', '^mp', 'damage;wpd;1600', 'm4.0', 'dustpig', 'parley\\x89ûªs', 'wackoe', 'mwednesday', '/friggin/', 'inj', 'pantofel', 'bookslast', 'colnharun', 'namjoon', 'bkn025', 'shevlin', 'ironmanå¨', 'bankstown', 'mi-7', 's\\x89ûªarabia', 'akx', '53inch', 'wall\\x89û\\x9d.', 'onlinea', 'turdnado', '0npzp', 'gt;.&gt', 'phandom', 'trynna', 'tejc', 'bg-16', 'kanglaonline', 'destroyd', 'people?!communication', 'roh3', 'hopfer', 'willingheart', 'foxa', 'amp;mdash', 'dardanelle', '\\x89ûïwhen', 'bkn032', '.@kurtschlichter', 'bhavana', 'wsjthinktank', '-set', 'windstorm.follow', 'rezaphotography', 'antiochhickoryhollow#tn', '4:28pm', 'nankana', 'she\\x89ûªs', 'tkyonly1fmk', 'dotish', 'nno', '\\x89ûïthat\\x89ûªs', 'zehr', 'clothesless', '-07', '\\x89û÷nother\\x89û\\x9d.', 'pbcanpcx', 'wordk', 'west\\x89ûªs', 'crush??', 'trancy', 'cause--', 'disowns', 'tribez', 'other\\x89û\\x9d.', 'imagini', 'wpri', '9km', 'funtenna', 'prettyboyshyflizzy', 'kasabwe', 'bosphore', 'maca', 'idis', 'gt;&gt;this', '8/6/15', 'cor4:8', '2leezy', 'lookg', 'crptotech', 'neil_eastwood77', 'todayhave', 'saumur', 'jaileen', '.@bbcnews', 'cins', 'lulgzimbestpict', 'cislady', 'sfor', '\\x89ûï@leoblakecarter', 'scaligero', 'ms++++++', 'linerless', 'bronville', 'dawabsha', 'i.s.i.s.', '40-pg', 'follownflnews', 'bioterror-', 'durvod', '52.214904', 'paci?c', 'wmur9', 'byityf', 'hem-712c', 'brbrs', '40mln', 'odai', 'fukurodani', '08/07/2015', 'factsheet', 'crashsterling!who', 's61.231a', 'lastma', 'you\\x89û\\x9d', 'kbak', 'mafireems', 'nowplaying', 'produc', 'gt;:33333', 'harm//', 'm5.5', 'grumpout', 'benoth', 'ocalan/', 'mesick', 'wednes', 'detonate&amp;shot', 'damage\\x89û\\x9d', 'taungbazar', 'mixxtail', 'say\\x89û\\x9d', 'indah', 'tweetstorm', 'shootoutåê', 'am@so@sorry', 'measures#arrestpastornganga', 'tier/', '.@uber', 'aåêmiddle', 'phalaborwa', 'wrk', '119000', 'gt;&gt;&gt', '6.beyonce', 'cau', 'pulkovo', '7:10pm', 'gray', 'r5live', 'injured?then', 'riveeeeeer', 'rare#deals_uk', 'trouble@niallharis', 'sirmione', \"5'41\", 'reator', 'rt_america', \"you'd-\", 'jbumzqpk', 'traumatised', 'disembarkment', '01:01:56', 'campanha', 'usatoday_nfl', '295ss-100', '22.beyonce', 'wiwnpfxa', 'ooohshit', 'bang&amp;my', '060/5', 'a.s.k', 'shoook', 'rindou', 'droppd', 'x1392', 'news@', '06:34:24', 'mcourt', 'gamescom', '.sink', 'lt;gasp!&gt', 'abused+desolate&amp;lost', 'aren\\x89ûªt', 'atåêcinema', 'ekiti', '27w', 'okanowa', '165000', 'amsal', 'tsutomi', 'pos', 'indianperpetrat', 'pageshi', '8/6/2015@1:08', 'throwingknifes', '18:26:24', 'fasting', '0853', \"todayi'm\", 'the_af', 'wouldn\\x89ûªt', '\\x89û÷body', 'togthe', 'foxsportscom', 'apocalpytic', '\\x89û÷alloosh', 'crosed', 'intertissue', 'doessnt', 'warnings.[900037', 'bigåêconsequence', 'responders/', '#pandemonium', 'shape.and', 'xkdrx', 'dorret', '.@runjewels', 'fuckface@wineisdumb.com', 'montana\\x89ûªs', 'mxaaaa', 'assertative', 'donå«t', 'dog\\x89ûªs', 'dubloadz', '2/his', '\\x89ûïyou', 'recordand', '2-car', '2slow2report', 'rt@irishirr', 'favs', 'jakeåê', '.@libertygeek83', '8550013', 'jrowah', 'avysss', '\\x89û÷amino', 'emergency', 'psalm34:22', 'ep03', 'sn', 'gameofkitten', 'lorr', 'geek_apocalypse', 'understand\\x89û\\x9d', 'zmne', '\\x89û÷leaf', '8015', 'dirumah', \"rock'n\", 'pp-400dr', 'rembr', 'moves:/', 'akxbskdn', 'humaza', '/via', \"it'sllikely\", 'seat|', 'm?.?o?.?p', 'klimkin', '8591', 'nside', '-07:0', 'butterlondon', 'ooooohshit', 'articals', 'gaelite', 'sedar', 'men\\x89ûªs', 'cobes', 'cnmi', 'ahamedis', 'tlvfaces#auspol', 'ambulance.we', \"s'th\", 'lindenow', 'wsls', 'psfda', '48/1', \"\\x89û¢åê'demolition\", 'hpssjd', 'tlvface', 'vallerand', '08/3/15', 'sunk\\x89û_1', 'mygc', 'motorcraft', 'ash@wo', '106:38', 'r3do', '08/02/15', '.@aiginsurance', '23:40:21', 'photoset', 'ofclans', \"lamp.':http://t.co/724gq5ebqz\", 'famine[mega', 'us101', 'morebut', 'akilah', '\\x89ã¢', 'brinx', 'butiqob', 'america\\x89ûªs', 'heavydirtysoul', 'bkb066gp', 'enugu', '-stacy', 'rchs', 'yiraneuni', 'freyas', 'alabamaquake', 'zarry', 'cw500', 'policylab', 'lgl', 'worldoil', 'detectado', 'kamayani', 'bluedio', 'fpoj', '-tune', 'harwich', '|via', 'bioterrorism', 'ng2x5', 'evildead', '08/06/15', 'he\\x89ûªs', '4sake', 'brenas', 'banki', 'tarmineta3', 'whyor', 'yamashiro', 'nrc_middleeast', 'apcåêpdp', 'e1.1.2', 'bigamist', 'are|delug', 'ofnsixjk', 'kindermorgan', 'newave', '-hubert', 'voortrekker', 'wed.aug.5th', 'spsgsp', 'ten-4', \"o'cuana\", 'gbbo', 'ep18', '-08:0', 'hrsto', 'festac', 'totteham', 'thursd', 'splatdown', 'candylit', 'commoditiesåêare', '375.000', 'ssu', 'our_mother_mary', 'relaxinpr', 'bayelsaåêstate', 'warriorcord', 'landsli', '85885473', 'daem', 'futurea', 'root', 'guimaras', \"36'x36\", 'knobhead', 'mwjcdk', 'ft.åêm.o.p.', 'shedid', 'purdie', 'independent-3', 'dorrian', 'us70', '\\x89ûïafter', 'rgj', 'xl61', 'x1386', 'amp;/or', 'ihhen', 'cc', 'hwo', 'romanatwoodvlog', 'office\\x89ûªs', 'protoshoggoth', 'followback', 'marketingåêmediocrity', 'pennlive', 'udhampur', 'hw18', 'kowing', 'ks100', '\\x89û÷it\\x89ûªs', 'elanorofrohan', 'niggas', 'xrrlnhelap', 'tozlet', '2-u.s.', 'jefferson&amp;doris', 'gards', 'gansey', 'zarharzar', 'spaceshiptwo', 'streamyx', 'chandrababu', '^sj', '\\x89ûïlove', 'janenelson097', 'wompppp', 'bamenda', '\\x89û÷it', '-legion', 'molecularly', 'court\\x89ûªs', 'nout', 'uvopwz', 'lt;a', 'whelen', 'ahahahga', 'boat.#new', 'sportwatch', 'bigstar', 'albertans', 'weallheartonedirection', 'magnitudeåêåêml', 'hiroshima.70000', 'noemotion', 'capsizes', 'chandanee', 'softenza', 'ìñ1', 'italianåêalp', 'barra', 'ashrafiyah', '05th', 'globi_inclusion', '030/6', 'fph01u3eii\\x89û\\x9d', 'misfortunebut', '238.00', '7a-7:30p', 'breakth', '17-jul-2015', 'kijan', 'amp;amp', 'marians', '\\x89ûï@bbcwomanshour', 'hollyw', 'disaster&amp', 'psqd', 'franta', '8:29pm', 'gwatt', 'colleenmnelson', 'tonight\\x89ûªs', ':-)))', 'ioqm5bm1dg', 'zakuun', \"\\x89û¢i'm\", '111020', 'ravioliåêwith', 'kotolily', \"took'em\", 'boy\\x89û\\x9d', 'khq', 'vosloorus', 'washard', 'kabwandi', 'ice&gt;&gt;&gt;&gt', '-popular', 'fighterdena', 'zonewolf123', '.@wwp', 'saferåê', 'shadowman', '03/08/11', 'ripriprip', 'i405', 'dh30000', 'regr', 'lamha', 'interesting!--why', 'charmeuse', 'r1354', 'fitba', 'deadgirltalking', '\\x89ûïhatchet', 'otp', '3la', 'transporta', '.@no_periferico', 'suruc', '-08:00', 'sms:087809233445', 'emmerdale', 'crbzfz', '-@entershikari', 'thruuu', 'jet\\x89ûªs', \"cont'd-\", '.@cityofcalgary', 'petition!take', 'lzk', 'x1402', 'rì', 'screams~', '.@aphl', '3\\\\:30a', 'brzjc', 'lanjut', '1:87', '7.beyonce', 'i\\x89ûªd', 'renewsit', 'rs40000cr', '\\x89ââ', 'thisisfaz', '\\x89ûï@leejasper', '8/6/2015@2:09', 'manåêarm', 'ssshhheeesshh', 'prepared!electrocutedboil', 'ms+++++', 'e(oficial', 'oworoshoki', 'sittway', '5.139055', 'connecto', 'lbr', 'khuzdar', 'zicac', '\\x89ûïplan', 'browseråêhijacker', 'intersectio', 'eversafe', '\\x89û÷islamic', 'skywars', 'pin:263789f4', 'ny1burst', 'king\\x89ûªs', 'cipinang', 'ruebs', 'denier-', 'rightways', 'piner', '4yygb', 'fastpitch', 'mbataweel', '19:36:35', 'frusciante', 'w(rec)k', 'mipasho', 'more\\x89ûó', 'm78ir0ik01\\x89û\\x9d', 'onr', 'amcx', 'ld0uniyw4k', '3-alarm', 'macia\\x89ûªs', 'rooms?', 'withåêannihilation', 'cbs3', 'fousey', 'pin:23928835', 'weren\\x89ûªt', '7aal', '.@slosheriff', 'games.https://t.co', 'kurtkamka', 'szuter', 'wpt-410', 'hasåêarrived', 'daubt', 'nh-3a', 'sunday\\x89ûªs', 'hatzolah', 'hotboy', 'cop', 'hirochii0', '.@raniakhalek', \"å¬'only\", 'buddys', 'drayesha4', 'bwin', '9:15pm', '7:13pm', 'zergele', 'sirens&amp', 'swanger', 'wolforth', 'xxhjesc', 'nockthie', 'fangirling', 'weiqin', '.haha', 'nasahurricane', 'apperception', 'rowyso', '2007he', 'hamayun', 'susiness', 'japton', 'raì¼l', 'id', 'bago', '2:22pm', '\\x89û÷minimum', '\\x89û÷nuclear', \"withdraw:'ur\", 'prysmian', 'saadthe', 'hy5pbe12tm', '0840728', '\\x89û÷badge', 'dhsscitech', 'signatures.change', 'unfortunemelody', '\\x89ûó', 'alisonannyoung', 'l9ud', \"'the\", 'musicvideo', 'edcxo', '\\x89ûª93', 'indyfest', 's3xleak', 'okayyyyyy', 'emsc', 'landolina', 'joboozoso', 'clelli', 'amp;got', 'sizygwwf', 'smugglersåênabbed', 'amazon\\x89ûªs', 'pochette', 'so?#japan', 'rnk', 'thda', '\\x89û÷food', 'yuvi', 'sixpenceee', 'warthen', '06:32:43', 'amp;start', '97/georgia', 'zw5jp46v5k\\x89û\\x9d', \"screams*don't\", 'm.o.p-', 'x1411', 'abbruchsimulator', 'postexistence', 'wiedemer', 'shawie17shawie', 'africaå¨', 'lhh', 'soloquiero', 'apksnplxzj', 'amp;#039;monster&amp;#039', '4103', 'cmcsa', 'commente', 'siouxland', 'elwoods', 'shidddd', '17:52:25', 'akwa', 'bestnaijamade', 'floorburnt', 'skanndtyagi', 'quantit\\x89û_https://t.co/64cymg1ltg', '.martinmj22', 'tarynel', 'packåêfor', 'iger\\x89ûªs', \"long'@foxy__siren\", 'trailheads', 'home2', 'arfur', 'enablement', 'atamathon', '\\x89ûï@based_georgie', '\\x89ûï@_keit', 'dserialpcilp', '516/leonardville', 'ems1', 'v4.5.2', 'å£9', 'john\\x89ûªs', 'shark\\x89û\\x9d', 'tecno', 'farmr', 'scghl0piq6', 'postapocalypticflimflam', 'selfavow', 'mirad', 'bowknot', 'usatoday', 'kwaaaaa#dead', 'khrone', '9:45pm', 'insurers163;million', 'naypyidaw', 'disaster?', '\\x89ûï', 'horrormovies.ca', 'bangtan', '\\x89û÷heat', 'you\\x89ûªll', 'windstorm', 'control?\\x89û\\x9d', \"1984'-style\", 'sherfield72', '#yr', 'viab', 'isn\\x89ûªt', 'alska-', 'wasn&amp;#8217;t', 'liar-#abbott', 'silentmind', 'dilawri', 'liveleakfun', 'kontrolled', 'kamon', 'jariana', \"i'm.sort\", 'reportly', 'lt;3', 'jhaustin', 'istg', 'lt;.75', 'siteinvestigat', 'berlatsky', 'sa-15', 'worldpay', 'teduka', 'pleasure~', 'that\\x89ûªs', '5km', 'die&gt', 'perrie', '.@stacdemon', 'alphen', 'åç', '6615434', 'news@@', 'sr37', 'versethe', 'bbcnew', 'scourgue', \"href='http://t.co\", '2pm', 'enemity', '\\x89û÷plot', 'byr', 'waterfur', 'destruction\\\\', \"rts'&amp;'democracy\", 'wrightsboro', 'hi\\x89ûótech', '\\x89û÷em', 'zenit', 'undergroundbestsellers', 'kc5kh', 'fennovoima', '\\x89û÷ill', 'eudrylantiqua', 'informant\\x89ûªs', 'we\\x89ûªre', 'yhngsjlg', 'ahhhhh', 'anthelmintic', 'swingman', 'detersion', 'damage;3460', '72w', '.potus', 'reafs', 'tianta', '.@norwaymfa', 'speculatio', 'syjexo', 'epicentre', 'jeepåêsunk', 'seeweed', 'tiffanyfrizzell', 'ebb', 'p.o.p.e.', 'popularmmos', '00:52:25', 'gpv', \"pay';pile\", 'fires-', 'clicca', 'grahamwp', 'chonce', 'today\\x89ûªs', 'onshit', 'exp0sed', 'bigrigradio', 'more--&gt;osha', 'carryi', 'oup', 'way;fieldstone', '.@fedex', '.@bigperm28', '\\x89ûïall', 'wctv-35', '23:54:09', 'pulwama', 'wroug', '-iowa', \"long'll\", 'humanityi', 'petebests', 'abbswinston', 'ikpeazu', 'didn\\x89ûªt', 'guabxfv', 'blackburn', 'hunterston', '\\x89ûï@thehighfession', '90blks&amp;8whts', 'it.\\x89û\\x9d', '\\x89ûïnews\\x89û\\x9d', 'becyme', 'warned\\x89û\\x9d', 'it\\x89ûªs', 'london3', 'arachys', 'we\\x89ûªve', 'noonan@cindynoonan', 'distance//', 'regc', '\\x89ûïthe', '30pcs', 'm1.57', 'costs-', 'm1.94', 'lt;&lt', 'drake\\x89ûªs', 'inec', 'fidayeen', 'etoffe', 'ladins', 'ush', 'darsena', '\\x89ûïnobody', '11:30bst', '//scream', 'fundwhen', 'me&amp;beat', '-10:00', 'inroices.|make', '28700', 'dqsvyusy', 'stay--', 'jax(mk2', 'itrawcwb', 'bromleythe', '4500-feet', 'wounded\\x89û\\x9d.', 'politifiact', 'mkayla', '18:22:45', 'oc73x', 'bomd', 'raffi_rc', 'utc]?3', 'you\\x89ûªre', 'ep016', 'naayf', '5:15p', \"kidnap'g.@afp\", '91å¡f', 'esevu', 'amp;ask', 'eurocrisis', 'stop!\\x89û\\x9d', 'boylesports', 'dorette', '06jst', 'hakogaku', 'clipuri', 'oper', 'kcarosawmur', 'shouout', 'volgagrad', 'x1441', 'ferguson\\x89ûªs', '\\x89û÷let\\x89ûªs', 'daviesmutia', 'vaping101', 'pathfinders', 'headlinelike', 'rt_com', 'bukidnon', 'followers-', 'precisionistic', 'crush?#mtvhott', 'maailiss', 'åêi', 'sorrower', 'time2015', '17months', 'catechize', 'don\\x89ûªt', 'j&amp;amp;k', '15901', 'ppor', 'tomorrow\\x89ûªs', 'convivt', 'umntu', 'leedstraif', 'follow', 'cerography', 'maddddd', 't1000', 'marketforce', '2015.08.06', 'knowlddge', 'lzkely', 'firefightr', 'slosher', '\\x89û÷british', '8437150124', 'suruì¤', 'peeps-', '\\x89ûïairplane\\x89û\\x9d', 'we\\x89ûªll', 'godslove', 'ziuw', 'ldnr', '.@david_cameron', 'responders-', 'l.victoria', '12jst', 'karymsky', 'lt;-----', 'wahpeton', '100nd', 'person', 'urgentthere', 'merle.--', 'pletch\\x89ûªs', 'stavola', 'soner', 'notificationsu', 'andåêchina', 'onlin', '400000\\x89ûò800000', 'secured-&gt', 'inciweb', 'thalapathi', '--head', 'scoopit', 'foragesecret', 'å£279.00end', 'socialtimes', 'x', 'wedneday', 'queenmy', '8/6/2015@1:32', 'china\\x89ûªs', 'kijima_matako', 'å£100bn', 'i\\x89ûªm', 'us?bush', 'crushed.2', 'tuffers', 'efak', 'ignoranceshe', 'clico', 'victimiser-#dutton', 'omw', 'massacre-4', 'torrecilla', 'damaturu', 'htarvrgly', 'hlongwane', 'marketforce\\x89ûªs', '.@vagersedolla', 'lt;-', 'yobe', 'salopek', 'chann', 'soultech', 'damage;nhs;999', 'ufn***-', 'ehek', '01:20:32', 'poconorecord', \"afghetc='left\", 'boyhaus', 'minister?says', 'here--&gt;cnn', 'goonda', 'disea', 'croydonization', 'horndale', 'demonstratio', 'deck\\x89û\\x9d', 'sevenfigz', 'pwhvgwax', 'kpdied', '14-hours', '/shower', 'foodstand', 'shtap', '-sigmund', 'tte', 'aaarrrgghhh', 'littlebitofbas', '\\x89ûïwe', 'a2&gt;hanover', 'tyar', 'firefigther', 'sigalert', 'ph0tos', 'mataas', 'ìñ', 'wpt-994', 'cluei', 'law\\x89ûónegligence', 'electrocute', '34:22', 'techniqu', 'shania', 'verhoek', 'alwsl', 'treatman', 'lancasteronline', 'jiahahahha', 'stephenscifi', 'heavyåêrain', 'intragenerational', 'embra', '2/n', 'uprootin', 'lotg', 'motordom', 'corii', 'let\\x89ûªs', 'belt?mr', 'maxsys', 'zaman', 'bajrangi', 'ks111', 'kwwwkwwwk', 'olap', 'trend.\\x89û\\x9d', 'fforecast', 'carolinaåêablaze', 'crozes', 'roomr', '01:02:17', 'riots-', 'n15b', 'safsufa', 'cryibg', 'dcfd', 'elvia', '.forbes', 'iclown', 'marquei', 'apga', 'dalroy', 'åè', 'ibeto', 'techesback', '1/2007', 'rabidmonkeys1', '241487', '08.05.15', 'icaseit', 'firefighte', 'listenlive', 'å¨', '05.08.2015', '\\x89ûïi', '325ci', 'aggressif', 'disclo', 'speedtech', 'mad+', 'cultsierre', 'emptive', 'mineness', 'mypillowstudio', '\\x89ûïnumber', 'city&amp;3other', \"them'@ayhoka\", '6773', '15:41:07', '16550', 'ks315', 'mediterran', 'gmmbc', 'smantibatam', 'kalmikya', 'schoolboy\\x89ûªs', 'll//ll=', 'colomr', '8/5/2015', 'lalaloopsy', '~still', 'ibom', 'wsoc', 'date', 'amp;#163;163;million', 'f$&amp;@', 'crapgamer', 'acesse', '/server', '-kfvs12', 'zabadani', 'aiii', 'sbee', 'j.a', 'lt;p&gt;an', 'graywardens', 'dorett', 'sabotage!i', '01:26:24', 'sinistras', '45600', 'dessicated', 'conditionsprivation', 'wbre', 'blaaaaaaa', 'unkn', 'troye', 'rebahe', 'requa', 'guard~', 'windstormåêinsurer', 'backty', 'president\\x89û\\x9d', 'beloeil', 'reliefweb', 'brinco', 'o784', 'vassalboro', 'patrickjbutler', 'casualties.\\x89û\\x9d', 'antipozi', '\\x89ûï@bbcengland', 'childfund', 'wasn\\x89ûªt', 'janeenorman', 'wonderkid', 'wowo--===', 'you\\x89ûªve', '-07:00', 'saudiåêmosque', 'fwo', '\\x89û÷faceless', 'death2usa', 'rescue~', '~3:45pm', 'spså¨', 'emittin', 'canadasuicide', 'spo', 'nail(real', 'uruan', 'oliviamiles01', 'nbanews', '08:02pm', 'hudhud', 'diaporama', 'gt;maintenance', 'broke=', 'inws', '8-minute', 'worseit', 'gnwt', '44:22', 'ancop', 'push2left', '1:04:01', 'm27329', '\\x89ûï@lolgop', 'dnbheaven', 'massgrave', 'ireporter', 'smoochy', 'mhtw4fnet', '8/5/15', 'å£150', 'fightåêterrorism', 'learni', 'karnal', 'hlg', 'israel\\x89ûªs', 'x-37b', 'defensenews.comus', 'onå¡å', 'koz', 'morning&amp;i', '4-suspected', 'dmpl', '^oo^', 'chpsre', 'recap/', 'buted', 'telly&amp;i', 'woman\\x89ûªs', 'fotoset', 'forrestmankin', 'katunew', 'walerga', 'åê', 'ronge', 'freebesieged', 'croydon', 'm3.8', 'samel_samel', '\\x89û÷muslim', 'gt;server', '1000&amp;1000', 'achimota', 'raheel', 'kisii', 'sr22', '1200000', '\\x89ûï@splottdave', 'hers?@vp', '260th', 'aaaaaaallll', 'molave', 'superv', '707d', 'macia', 'aannnnd', 'robot_rainstorm', 'sidjsjdjekdjskdjd', 'meelllttt', 'pt4', 'amp;story', \"can'twon't\", 'gtxrwm', 'subsd', 'catalogue~', 'zrnf', 'urogyn', 'lizzie363', '15:04:01', 'totoooooo', 'courtof', 'sparkz', '21:1023', 'nbcnightlynews', 'å£300', 'wyou', 'wwa', 'jawans', 'slideshare', 'damatu', 'whensoever', '124.13.172.40', 'they\\x89ûªd', '-los', 'isis.i', 'latinoand', 'ckec', 'parsholics', 'faceåê(photo', 'nkea', 'copperfields', 'newidea', 'santanico', '138:8', 'ashenfor', 'legionna', \"cactus?'#borderland\", '\\x89ûïrichmond', 'deejayempiresound', 'ohh', 'inspi', '20+homer', 'iembot_hfo', 'damiengayle', 'apch', 'hunhry', 'disney\\x89ûªs', 'news786-uk', 'dragonvale', 'painthey', 'nitty', 'm-115', 'driving.\\x89û\\x9d', 'bfore', 'younger&amp;grossly', 'hawk)!his', 'afycso', 'err:509', 'k_matako_bot', 'buthidaung', 'ft/7p-7a', 'tìüp', '.@unsuckdcmetro', 'bickleton', 'riooooos', 'kesabaran', 'notexplained', 'mom-*die', 'amazondeal', '7amdollela', 'benstracy', 'gt;_&gt', 'nyozi', 'mccain\\x89ûªs', 'lifeit', 'lmaov.v', 'carhot', '\\x89ûókody', 'cbs29', 'vtc', 'ohlordy', 'jaiden', 'homefolk', 'dr;08/05/2015', 'outbid', '85v-265v', 'lzktjnox', 'molys', 'noxdv', 'whitbourne', 'sif-', 'zotar(50', '2.iclown', '.@karijobe', '#yycstorm', 'hostage&amp;2', '\\x89ûïstretcher', 'cafì', 'gangstermail', 'derailed_benchmark', 'pin:2327564d', 'arceen', 'yeyeulala', 'duckvillelol', 'ayhhhhhdjjfjrjjrdjjek', '08/06/2015', 'home.@abc', 'offr', '\\x89ûï@fdny', 'wheels-(io', 'hamptonroadsfor.me', 'inbounds/', 'swea', 'leg)walked-', 'babalmao', '\\x89ûï@lordbrathwaite', 'shocking.\\x89ûï', 'wftv', 'stepkans', 'since-3', 'bebacksoon', 'crqck', 'illusoria', 'fdbdp', 'help.#arsenal', '10:40pm', 'apaz', '.@robdelaney', 'overåêhostage', \"fine'-me\", 'mfs????????loose', '\\x89ûï@dylanmcclure55', 'avalanches&gt;&gt;https://t.co', 'practice?#nyg', 'buildings\\x89ûówe', 'salyers', 'earthquake.~', 'trinna', 'w|', 'east\\x89ûªs', 'gt;_&lt', 'dialysis', 'chrissie', '.@dantwitty52', 'lt;b&gt;bake&lt;/b&gt', 'inåêchaos', 'ndetention', 'briliantly', 'sustainourearth', 'owenrbroadhurst', 'serephina', 'hardside', 'saudies', 'gyrsi', '\\x89ûóher', 'multidimensi', 'matchwood', 'sarumi', 'volcanoåêinåêrussia', 'demco', 'timesofindia', 'i\\x89ûªve', \"married'@foxy__siren\", 'famineåêmemorie', 'dyxtmrydu', 'utc]?5', 'linkury', 'sa15', '911bombing', 'gt;as', 'lt;b&gt;chocolate&lt;/b&gt', 'femnism', 'hendrixonfire', 'enca', 'sniiiiiiff', 'lavenderpoetrycafe', 'langtree', 'mido', 'dualcom', 'bioterrorism?@ap', 'radneck', 'weatherit', 'navbl', 'wallybaiter', 'dorrie', 'buchana', 'unsensibly', 'fittscott', 'dougkessler', 'awesomeeeeeeee', 'everythign', 'yyeso', 'ihhrkg4v1s.', 'trfc', '16:54:09', '\\x89ûïa', 'saddlebrooke', 'bb4sp', 'qzloremft', 'vabengal', '//kinda', 'åêfedex', 'agnivesh', 'fiasco-', '17.beyonce', 'ofr', '|lauren', 'xekstrin', 'siouxlan', '\\x89û÷institute', 'old', 'utc2015', 'murlo', 'arena.none', 'fylde', '11:30a', '6aug', 'jaylenejoybeligan', 'ìü', 'air1bullet', 'nagaski', 'ltz', 'ssw', '.@greenbuildermag', 'alaska\\x89ûªs', 'liqd', 'friend59', '14028', 'womem', 'shanghai\\x89ûªs', 'wyrmwood', 'mothernaturenetwork', 'forestservice', 'fahlo:#wcw', 'sometimesi', 'aashiqui', \"cos'it\", 'miprv', 'a10&gt;paris', 'hermancranston']\n"
     ]
    }
   ],
   "source": [
    "print(list(filtered_oov2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do seem to have the insight now that '\\x89ûª' can be replaced with a single quote in most cases (though this doesn't fit exactly with shortening everything to lemmas, but is probably a better compromise than removing the tokens outright), and a few tokens that can likely be saved by removing prefix '\\x89û÷' and '\\x89ûï'. There are some typos, but since we're looking at something generalizable at the moment, we won't manually correct anything. We do spot a few more tokens that can be saved with patterns though: some words ending with '?' and some words starting (or ending) with '/' or '//'. We'll thus conduct our final pass saving tokens that match in this way, and letting all other oov tokens be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def keep_token(tok):\n",
    "    \"\"\" Filter out tokens without predictive value \"\"\"\n",
    "    if not usable_token(tok) or tok.lemma_ == 'ûª':\n",
    "        return False\n",
    "    \n",
    "    return ((not is_oov(tok)) or re.search('\\x89û', tok.lemma_)\n",
    "            or tok.lemma_.startswith('/') or tok.lemma_.endswith('/')\n",
    "            or tok.lemma_.endswith('?'))\n",
    "\n",
    "def trimmed_lemma(lemma):\n",
    "    trimmed = lemma.replace(\"\\x89ûª\", \"'\")\n",
    "    trimmed = lemma.replace(\"\\x89û÷\", \"\")\n",
    "    trimmed = lemma.replace(\"\\x89ûï\", \"\")\n",
    "    trimmed = lemma.replace(\"\\x89û\\x9d\", \"\")\n",
    "    trimmed = lemma.replace(\"\\x89û\", \"\")\n",
    "    trimmed = lemma.replace(\"/\", \"\")\n",
    "    trimmed = lemma.replace(\"?\", \"\")\n",
    "    return trimmed\n",
    "\n",
    "def preprocess(nlp, tt):\n",
    "    parsed = nlp(tt)\n",
    "    return \" \".join([left_hash(token) + trimmed_lemma(token.lemma_) for token in parsed\n",
    "                     if keep_token(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_tweets = tweet_df.text.apply(df_nlp_app(nlp, preprocess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick consistency check on our process so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>-PRON- deed reason #earthquake allah forgive -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive #wildfire evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>send photo ruby #alaska smoke #wildfire pour s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>#rockyfire update = &gt; california hwy 20 close ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>#flood #disaster heavy rain cause flash floodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>-PRON- hill -PRON- fire wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>emergency evacuation happen building street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>-PRON- afraid tornado come -PRON- area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1             Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are ...   \n",
       "3  13,000 people receive #wildfires evacuation or...   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  I'm on top of the hill and I can see a fire in...   \n",
       "8  There's an emergency evacuation happening now ...   \n",
       "9  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "                                           processed  \n",
       "0  -PRON- deed reason #earthquake allah forgive -...  \n",
       "1                    forest fire near la sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3  13,000 people receive #wildfire evacuation ord...  \n",
       "4  send photo ruby #alaska smoke #wildfire pour s...  \n",
       "5  #rockyfire update = > california hwy 20 close ...  \n",
       "6  #flood #disaster heavy rain cause flash floodi...  \n",
       "7                       -PRON- hill -PRON- fire wood  \n",
       "8        emergency evacuation happen building street  \n",
       "9             -PRON- afraid tornado come -PRON- area  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'original': tweet_df.text.head(10), 'processed': processed_tweets[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spacy preprocessing seems to have added in '-PRON-'s of unknown origin, we'll remove these before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = [tweet.replace('-PRON- ', '') for tweet in processed_tweets]\n",
    "processed_tweets = [tweet.replace(' -PRON-', '') for tweet in processed_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason #earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive #wildfire evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>send photo ruby #alaska smoke #wildfire pour s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>#rockyfire update = &gt; california hwy 20 close ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>#flood #disaster heavy rain cause flash floodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>hill fire wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>emergency evacuation happen building street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>afraid tornado come area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1             Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are ...   \n",
       "3  13,000 people receive #wildfires evacuation or...   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  I'm on top of the hill and I can see a fire in...   \n",
       "8  There's an emergency evacuation happening now ...   \n",
       "9  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "                                           processed  \n",
       "0              deed reason #earthquake allah forgive  \n",
       "1                    forest fire near la sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3  13,000 people receive #wildfire evacuation ord...  \n",
       "4  send photo ruby #alaska smoke #wildfire pour s...  \n",
       "5  #rockyfire update = > california hwy 20 close ...  \n",
       "6  #flood #disaster heavy rain cause flash floodi...  \n",
       "7                                     hill fire wood  \n",
       "8        emergency evacuation happen building street  \n",
       "9                           afraid tornado come area  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'original': tweet_df.text.head(10), 'processed': processed_tweets[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And before moving on, now that we've done some initial cleanup, lets do another sweep for potential stop words that weren't in Spacy's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "for tweet in processed_tweets:\n",
    "    for word in tweet.split(' '):\n",
    "        words.append(word)\n",
    "        \n",
    "word_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 394),\n",
       " (\"'s\", 381),\n",
       " ('fire', 345),\n",
       " ('amp', 298),\n",
       " ('new', 226),\n",
       " ('people', 197),\n",
       " ('kill', 174),\n",
       " ('video', 170),\n",
       " ('burn', 166),\n",
       " ('2', 165),\n",
       " ('crash', 160),\n",
       " ('attack', 154),\n",
       " ('emergency', 153),\n",
       " ('body', 153),\n",
       " ('come', 151),\n",
       " ('disaster', 151),\n",
       " ('bomb', 149),\n",
       " ('year', 147),\n",
       " ('look', 145),\n",
       " ('day', 143),\n",
       " ('|', 142),\n",
       " ('good', 142),\n",
       " ('police', 141),\n",
       " ('man', 138),\n",
       " ('know', 138),\n",
       " ('time', 133),\n",
       " ('love', 129),\n",
       " ('family', 127),\n",
       " ('building', 126),\n",
       " ('flood', 126),\n",
       " ('think', 126),\n",
       " ('storm', 125),\n",
       " ('life', 122),\n",
       " ('home', 121),\n",
       " ('suicide', 120),\n",
       " ('news', 118),\n",
       " ('watch', 118),\n",
       " ('want', 117),\n",
       " ('train', 117),\n",
       " ('california', 116),\n",
       " ('car', 114),\n",
       " ('collapse', 114),\n",
       " ('death', 111),\n",
       " ('work', 110),\n",
       " ('3', 107),\n",
       " ('world', 104),\n",
       " ('scream', 102),\n",
       " ('today', 101),\n",
       " ('need', 100),\n",
       " ('let', 99),\n",
       " ('dead', 97),\n",
       " ('wreck', 97),\n",
       " ('old', 96),\n",
       " ('bag', 95),\n",
       " ('war', 94),\n",
       " ('nuclear', 94),\n",
       " ('accident', 93),\n",
       " ('destroy', 92),\n",
       " ('fear', 90),\n",
       " ('drown', 90),\n",
       " ('way', 86),\n",
       " ('help', 86),\n",
       " ('injury', 85),\n",
       " ('plan', 82),\n",
       " ('u', 82),\n",
       " ('@youtube', 82),\n",
       " ('blow', 82),\n",
       " ('find', 81),\n",
       " ('weapon', 81),\n",
       " ('use', 80),\n",
       " ('big', 80),\n",
       " ('live', 79),\n",
       " ('run', 79),\n",
       " ('survive', 79),\n",
       " ('bad', 79),\n",
       " ('report', 78),\n",
       " ('feel', 78),\n",
       " ('explode', 78),\n",
       " ('die', 77),\n",
       " ('5', 77),\n",
       " ('fall', 77),\n",
       " ('evacuate', 77),\n",
       " ('rescue', 76),\n",
       " ('collide', 75),\n",
       " ('mass', 74),\n",
       " ('panic', 74),\n",
       " ('derail', 74),\n",
       " ('wave', 73),\n",
       " ('right', 73),\n",
       " ('thing', 73),\n",
       " ('#news', 73),\n",
       " ('save', 73),\n",
       " ('2015', 72),\n",
       " ('army', 72),\n",
       " ('quarantine', 72),\n",
       " ('school', 70),\n",
       " ('sink', 70),\n",
       " ('hiroshima', 70),\n",
       " ('black', 69),\n",
       " ('lol', 69),\n",
       " ('late', 69),\n",
       " ('fatality', 69),\n",
       " ('wildfire', 68),\n",
       " ('4', 68),\n",
       " ('damage', 67),\n",
       " ('city', 67),\n",
       " ('great', 67),\n",
       " ('read', 67),\n",
       " ('stop', 67),\n",
       " ('fatal', 67),\n",
       " ('hear', 67),\n",
       " ('water', 67),\n",
       " ('crush', 67),\n",
       " ('leave', 66),\n",
       " ('pm', 65),\n",
       " ('bombing', 65),\n",
       " ('miss', 65),\n",
       " ('detonate', 65),\n",
       " ('forest', 64),\n",
       " ('northern', 64),\n",
       " ('wound', 64),\n",
       " ('mh370', 64),\n",
       " ('try', 63),\n",
       " ('cross', 63),\n",
       " ('obama', 63),\n",
       " ('demolish', 63),\n",
       " ('service', 62),\n",
       " ('hit', 62),\n",
       " ('confirm', 62),\n",
       " ('deluge', 62),\n",
       " ('electrocute', 62),\n",
       " ('bomber', 62),\n",
       " ('break', 61),\n",
       " ('set', 61),\n",
       " ('1', 61),\n",
       " ('woman', 61),\n",
       " ('obliterate', 61),\n",
       " ('head', 60),\n",
       " ('house', 60),\n",
       " ('hostage', 60),\n",
       " ('cause', 59),\n",
       " ('end', 59),\n",
       " ('s', 59),\n",
       " ('start', 59),\n",
       " ('reddit', 59),\n",
       " ('thank', 58),\n",
       " ('play', 58),\n",
       " ('atomic', 58),\n",
       " ('casualty', 58),\n",
       " ('near', 57)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.most_common(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And while it's hard to say whether some words will be more associated with disaster or non disastert tweets, we do see a few obvious tokens that seem to be without meaning and that we can further filter at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(processed_tweets)):\n",
    "    tweet_words = processed_tweets[i].split(' ')\n",
    "    processed_tweets[i] =\\\n",
    "        ' '.join([w for w in tweet_words\n",
    "                  if (w != \"u\" and w != \"2\" and w != \"4\"and w != \"'s\" \n",
    "                      and w != \"s\" and w != \"|\" and w != \"amp\" and w != '')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for one final step in preprocessing, before adding this processed column to our data frame, we'll ajoin likely bigram words using the gensim implementation of phrase modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "bigramed_tweets = Phrases(processed_tweets)\n",
    "processed_tweets = pd.Series(\n",
    "    [''.join(bigramed_tweets[tweet]) for tweet in processed_tweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can add the processed tweets to our original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_df['processed_text'] = processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason #earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive #wildfire evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>send photo ruby #alaska smoke #wildfire pour s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>#rockyfire update = &gt; california hwy 20 close ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>#flood #disaster heavy rain cause flash floodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>hill fire wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>emergency evacuation happen building street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>afraid tornado come area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1             Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are ...   \n",
       "3  13,000 people receive #wildfires evacuation or...   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  I'm on top of the hill and I can see a fire in...   \n",
       "8  There's an emergency evacuation happening now ...   \n",
       "9  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "                                      processed_text  \n",
       "0              deed reason #earthquake allah forgive  \n",
       "1                    forest fire near la sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3  13,000 people receive #wildfire evacuation ord...  \n",
       "4  send photo ruby #alaska smoke #wildfire pour s...  \n",
       "5  #rockyfire update = > california hwy 20 close ...  \n",
       "6  #flood #disaster heavy rain cause flash floodi...  \n",
       "7                                     hill fire wood  \n",
       "8        emergency evacuation happen building street  \n",
       "9                           afraid tornado come area  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do a sanity check on these entries\n",
    "tweet_df.head(10)[['text', 'processed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "And next we will use LDA modeling, to obtain the most likely topics of each tweet. Each tweet will be converted to a bag of words format before being passed to gensims LDA implementation. After which each topics words will be assessed to assign topic names to the derived topic groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokened_tweets = processed_tweets.apply(lambda tweet: tweet.split(' '))\n",
    "pt_dictionary = Dictionary(tokened_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pt_generator(pt_dict, tweets):\n",
    "    for tweet in tweets:\n",
    "        yield pt_dict.doc2bow(tweet)\n",
    "        \n",
    "MmCorpus.serialize(\n",
    "    '../data/pt_bow_corpus.mm',\n",
    "    pt_generator(pt_dictionary, tokened_tweets))\n",
    "\n",
    "tweet_mmcorpus = MmCorpus('../data/pt_bow_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda = LdaMulticore(tweet_mmcorpus,\n",
    "                       num_topics=25,\n",
    "                       id2word=pt_dictionary,\n",
    "                       workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda.save('../data/lda_topic_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of exploration shows us that it's difficult to derive names or themes for our topics though, likely due to the short length of using a text as short as tweets as documents. As a result we will save just the primary topic number as assigned by LDA to each of our tweets for possible predictive value, and potentially use our saved lda model for further exploration later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "time                 0.007\n",
      "siren                0.005\n",
      "terrorist            0.005\n",
      "trouble              0.005\n",
      "wound                0.005\n",
      "sink                 0.005\n",
      "road                 0.004\n",
      "trauma               0.004\n",
      "let                  0.004\n",
      "video                0.004\n",
      "face                 0.004\n",
      "kill                 0.004\n",
      "police               0.004\n",
      "sinkhole             0.004\n",
      "attack               0.004\n",
      "volcano              0.003\n",
      "fire                 0.003\n",
      "08                   0.003\n",
      "know                 0.003\n",
      "storm                0.003\n",
      "km                   0.003\n",
      "lot                  0.003\n",
      "test                 0.003\n",
      "survive              0.003\n",
      "west                 0.003\n"
     ]
    }
   ],
   "source": [
    "explore_topic(1, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "like                 0.007\n",
      "fire                 0.006\n",
      "good                 0.005\n",
      "sink                 0.004\n",
      "snowstorm            0.004\n",
      "storm                0.004\n",
      "3                    0.004\n",
      "video                0.004\n",
      "body                 0.004\n",
      "life                 0.004\n",
      "look                 0.004\n",
      "today                0.004\n",
      "siren                0.004\n",
      "wreck                0.004\n",
      "day                  0.004\n",
      "new                  0.004\n",
      "little               0.003\n",
      "@youtube             0.003\n",
      "cross                0.003\n",
      "bag                  0.003\n",
      "people               0.003\n",
      "shoulder             0.003\n",
      "car                  0.003\n",
      "half                 0.003\n",
      "need                 0.003\n"
     ]
    }
   ],
   "source": [
    "explore_topic(2, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics = [lda[pt_dictionary.doc2bow(tweet)][0][0] for tweet in tokened_tweets]\n",
    "tweet_df['primary_topic'] = pd.Series(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>primary_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason #earthquake allah forgive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la sask canada</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfire evacuation ord...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>send photo ruby #alaska smoke #wildfire pour s...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                     processed_text  primary_topic  \n",
       "0       1              deed reason #earthquake allah forgive              0  \n",
       "1       1                    forest fire near la sask canada             12  \n",
       "2       1  resident ask shelter place notify officer evac...              3  \n",
       "3       1  13,000 people receive #wildfire evacuation ord...             15  \n",
       "4       1  send photo ruby #alaska smoke #wildfire pour s...             17  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify state of dataframe\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "And with that we'll save out our modified dataframe before constructing our tf-idf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.to_csv('../data/processed_kaggle_training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Term frequency x Inverse Document Frequency is a representation similar to bag of words, but instead of being a direct count of term, it weighs each terms across their appearence in an entire corpus. This serves to add a measure of term importance to each term in a document when storing that bag of words doesn't have, as well as scaling the representations in each document vector to values that tend to yield better results when ran through various machine models.\n",
    "\n",
    "This time we will use a library provided by sklearn to vectorize our corpus before writing them to a seperate data file (this time for storage of a sparse matrix which is the typical form of this corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidvectorizer = TfidfVectorizer(min_df=4, max_df=0.8)\n",
    "Xtfidf = tfidvectorizer.fit_transform(tweet_df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "scipy.sparse.save_npz('../data/tweets-tf-idf.npz', Xtfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
