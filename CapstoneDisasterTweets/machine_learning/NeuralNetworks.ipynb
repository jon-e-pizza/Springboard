{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the advent of the word2vec model, and recurrent neural networks, we can use neural networks to derive further insight from the language of tweets on their disaster level over what typical bag of words models can give us with traditional machine learning techniques. We will use Keras to implement our neural network, and derive new word2vec word embeddings rather than use a prebuild set as we expect the use of language in tweets to be somewhat different than conventional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using our corpus of previously processed tweets\n",
    "import pandas as pd\n",
    "\n",
    "tweet_df = pd.read_csv('../data/processed_kaggle_training.csv')\n",
    "tweet_df['processed_text'] = tweet_df['processed_text'].astype(str)\n",
    "texts = tweet_df.processed_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare for skipgram word2vec with keras helpers\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "wids = [[word2id[w] for w in text_to_word_sequence(text)] for text in texts]\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=len(word2id)) for wid in wids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jon-e-pizza/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon-e-pizza/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\")`\n"
     ]
    }
   ],
   "source": [
    "# construct our keras model for skipgrams\n",
    "from keras.layers import Dot, Input\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "VOCAB_SIZE = len(word2id) + 1\n",
    "EMBED_SIZE = 100\n",
    "\n",
    "word_input = Input(name='word_input',shape=[1])\n",
    "layer = Embedding(\n",
    "    VOCAB_SIZE, EMBED_SIZE, embeddings_initializer=\"glorot_uniform\",\n",
    "    input_length=1)(word_input)\n",
    "word_layer = Reshape((EMBED_SIZE,))(layer)\n",
    "\n",
    "context_input = Input(name='context_input',shape=[1])\n",
    "layer = Embedding(\n",
    "    VOCAB_SIZE, EMBED_SIZE, embeddings_initializer=\"glorot_uniform\",\n",
    "    input_length=1)(context_input)\n",
    "context_layer = Reshape((EMBED_SIZE,))(layer)\n",
    "\n",
    "merge_layer = Dot(axes=1)([word_layer, context_layer])\n",
    "output = Dense(1, init=\"glorot_uniform\", activation=\"sigmoid\")(merge_layer)\n",
    "sg_model = Model(inputs = [word_input, context_input], outputs = output)\n",
    "    \n",
    "sg_model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jon-e-pizza/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 1 Loss: 1482.4700885526836\n",
      "Epoch: 2 Loss: 1094.4184742826037\n",
      "Epoch: 3 Loss: 897.1317858066177\n",
      "Epoch: 4 Loss: 662.4907645033127\n",
      "Epoch: 5 Loss: 480.5593598252046\n"
     ]
    }
   ],
   "source": [
    "# and now train our prepared skipgrams on our model\n",
    "import numpy as np\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        if len(elem[0]) == 0:\n",
    "            continue\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        loss += sg_model.train_on_batch(X,Y)\n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 387.7060118282038\n",
      "Epoch: 7 Loss: 279.3309925262673\n",
      "Epoch: 8 Loss: 238.44166559127513\n",
      "Epoch: 9 Loss: 172.69524405942457\n"
     ]
    }
   ],
   "source": [
    "# still seeing a pretty big drop in the loss at the end there, lets run\n",
    "# a few more epochs before saving the embedding\n",
    "for epoch in range(6, 10):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        if len(elem[0]) == 0:\n",
    "            continue\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        loss += sg_model.train_on_batch(X,Y)\n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
