{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Neural Networks\n",
    "\n",
    "In the space of NLP the word2vec model refers to a numerical vector representation of the meaning of words. The singular numerical representations of each word when modeling with this method are somewhat arbitrary, but they can provide a way by which words of similar meaning in a set of word vectors are expected to have vectors that are close to each other.\n",
    "\n",
    "Creating such vectors can be a good starting point either for deriving the meaning of unknown words, or deriving new value from the computed meanings. There are a number of methods that utilize neural networks and their back propagation in order to derive word vector representations. In our neural network approach we will first aim to use the skip-gram neural network design to derive a set of word vectors over our tweet space, and then we'll feed our word vectors into an algorithm for determining the overall sentiment (disaster/safe) of our tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "We'll be implementing the skip-gram algorithm over our corpus to derive our word vectors. In the skip gram method, words in a corpus are paired together with the output of 1 if they appear close to each other and 0 if they are just random pairs, with embedding layers linking a vocabulary size input to those binary outputs. The embedding layers will then become your word2vec vectors to potentially be applied to another model.\n",
    "\n",
    "It's important to train word vectors over the whole corpus, because we'll be training our classifier over just part of the corpus, and can't guarantee everything in the testing set will be in the training set, so we can at least get some predictive capability from the word vectors if we train the vectors we'll input on the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load our corpus of previously processed tweets\n",
    "import pandas as pd\n",
    "\n",
    "tweet_df = pd.read_csv('../data/processed_kaggle_training.csv')\n",
    "tweet_df['processed_text'] = tweet_df['processed_text'].astype(str)\n",
    "texts = tweet_df.processed_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# prepare for skipgram word2vec with keras helpers\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "wids = [[word2id[w] for w in text_to_word_sequence(text)] for text in texts]\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=len(word2id)) for wid in wids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jon-e-pizza/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon-e-pizza/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\")`\n"
     ]
    }
   ],
   "source": [
    "# construct our keras model for skipgrams\n",
    "from keras.layers import Dot, Input\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "VOCAB_SIZE = len(word2id) + 1\n",
    "EMBED_SIZE = 100\n",
    "\n",
    "word_input = Input(name='word_input',shape=[1])\n",
    "layer = Embedding(\n",
    "    VOCAB_SIZE, EMBED_SIZE, embeddings_initializer=\"glorot_uniform\",\n",
    "    input_length=1)(word_input)\n",
    "word_layer = Reshape((EMBED_SIZE,))(layer)\n",
    "\n",
    "context_input = Input(name='context_input',shape=[1])\n",
    "layer = Embedding(\n",
    "    VOCAB_SIZE, EMBED_SIZE, embeddings_initializer=\"glorot_uniform\",\n",
    "    input_length=1)(context_input)\n",
    "context_layer = Reshape((EMBED_SIZE,))(layer)\n",
    "\n",
    "merge_layer = Dot(axes=1)([word_layer, context_layer])\n",
    "output = Dense(1, init=\"glorot_uniform\", activation=\"sigmoid\")(merge_layer)\n",
    "sg_model = Model(inputs = [word_input, context_input], outputs = output)\n",
    "    \n",
    "sg_model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jon-e-pizza/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 1 Loss: 1546.904946116265\n",
      "Epoch: 2 Loss: 1097.2918282193132\n",
      "Epoch: 3 Loss: 723.0027144331598\n",
      "Epoch: 4 Loss: 435.4063223999115\n",
      "Epoch: 5 Loss: 268.8751482185553\n"
     ]
    }
   ],
   "source": [
    "# and now train our prepared skipgrams on our model\n",
    "import numpy as np\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        if len(elem[0]) == 0:\n",
    "            continue\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        loss += sg_model.train_on_batch(X,Y)\n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 183.66639455841906\n",
      "Epoch: 7 Loss: 124.08219412733192\n",
      "Epoch: 8 Loss: 93.6779270734111\n",
      "Epoch: 9 Loss: 69.78028212264012\n"
     ]
    }
   ],
   "source": [
    "# still seeing a pretty big drop in the loss at the end there, lets run\n",
    "# a few more epochs before saving the embedding\n",
    "for epoch in range(6, 10):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        if len(elem[0]) == 0:\n",
    "            continue\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        loss += sg_model.train_on_batch(X,Y)\n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We're in the realm of overfitting now, lets save these embeddings\n",
    "tok_words = [v for v in id2word.values()]\n",
    "tok_words.insert(0, 'PAD')\n",
    "\n",
    "weights = sg_model.layers[2].get_weights()[0][:]\n",
    "wdf = pd.DataFrame(weights, index=tok_words)\n",
    "wdf.to_csv('word_vectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now that we have some trained word vectors over our entire corpus, we can plug these in to our sequential classification model. This is an interesting deviation from traditional machine learning methods that typically have to operate solely on a bag of words input. Instead our machine learning should be able to utilize the context of words for extra predictive capability, though we'll have to compare this method and our traditional methods afterwards to determine which is superior on this input.\n",
    "\n",
    "The particular mechanism by which sequential context information is kept relevant will be obscured somewhat to us behind the Keras implemented Long-Short Term Memory cell. For our purposes we can know they're an implementation in the Keras layer that both keeps some memory of past values and works to help prevent the shrinking/exploding gradient problem of just stacking dense layers to predict the output of sequential input. We'll also be utilizing a random dropout layer provided by keras to prevent overfitting of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_input (InputLayer)      (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 26, 100)           1181200   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "lstm_output (Dense)          (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,282,001\n",
      "Trainable params: 100,801\n",
      "Non-trainable params: 1,181,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, LSTM, Dropout\n",
    "\n",
    "# we'll be padding all tweets to the max tweet's token length\n",
    "# for consistent sequence length\n",
    "INPUT_LEN = max([len(wid_v) for wid_v in wids])\n",
    "\n",
    "# define the model\n",
    "inputs = Input(name=\"lstm_input\", shape=[INPUT_LEN])\n",
    "layer = Embedding(\n",
    "    VOCAB_SIZE, EMBED_SIZE, weights=[weights], input_length=INPUT_LEN,\n",
    "    trainable=False\n",
    "    )(inputs)\n",
    "layer = LSTM(100)(layer)\n",
    "layer = Dense(200, activation='relu')(layer)\n",
    "layer = Dropout(0.2)(layer)\n",
    "layer = Dense(1, name=\"lstm_output\")(layer)\n",
    "layer = Activation(\"sigmoid\")(layer)\n",
    "lstm_model = Model(inputs=inputs, outputs=layer)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# compile the model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must make our sequences to feed to this model, as well as our output vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pad_sequences(wids, maxlen = INPUT_LEN, padding='post', value=0)\n",
    "y = tweet_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4872 samples, validate on 1218 samples\n",
      "Epoch 1/50\n",
      "4872/4872 [==============================] - 3s 528us/step - loss: 0.6081 - accuracy: 0.6712 - val_loss: 0.5891 - val_accuracy: 0.7184\n",
      "Epoch 2/50\n",
      "4872/4872 [==============================] - 2s 354us/step - loss: 0.5360 - accuracy: 0.7492 - val_loss: 0.5531 - val_accuracy: 0.7365\n",
      "Epoch 3/50\n",
      "4872/4872 [==============================] - 2s 381us/step - loss: 0.5112 - accuracy: 0.7599 - val_loss: 0.6208 - val_accuracy: 0.7438\n",
      "Epoch 4/50\n",
      "4872/4872 [==============================] - 2s 332us/step - loss: 0.5040 - accuracy: 0.7685 - val_loss: 0.5595 - val_accuracy: 0.7594\n",
      "Epoch 5/50\n",
      "4872/4872 [==============================] - 2s 382us/step - loss: 0.5024 - accuracy: 0.7683 - val_loss: 0.5647 - val_accuracy: 0.7209\n",
      "Epoch 6/50\n",
      "4872/4872 [==============================] - 2s 349us/step - loss: 0.4929 - accuracy: 0.7728 - val_loss: 0.5677 - val_accuracy: 0.7529\n",
      "Epoch 7/50\n",
      "4872/4872 [==============================] - 2s 340us/step - loss: 0.4850 - accuracy: 0.7806 - val_loss: 0.7172 - val_accuracy: 0.7217\n",
      "Epoch 8/50\n",
      "4872/4872 [==============================] - 2s 335us/step - loss: 0.4877 - accuracy: 0.7789 - val_loss: 0.5485 - val_accuracy: 0.7258\n",
      "Epoch 9/50\n",
      "4872/4872 [==============================] - 2s 337us/step - loss: 0.4807 - accuracy: 0.7796 - val_loss: 0.6028 - val_accuracy: 0.7537\n",
      "Epoch 10/50\n",
      "4872/4872 [==============================] - 2s 340us/step - loss: 0.4737 - accuracy: 0.7843 - val_loss: 0.5865 - val_accuracy: 0.7406\n",
      "Epoch 11/50\n",
      "4872/4872 [==============================] - 2s 358us/step - loss: 0.4659 - accuracy: 0.7880 - val_loss: 0.6483 - val_accuracy: 0.7529\n",
      "Epoch 12/50\n",
      "4872/4872 [==============================] - 2s 380us/step - loss: 0.4712 - accuracy: 0.7884 - val_loss: 0.7338 - val_accuracy: 0.7011\n",
      "Epoch 13/50\n",
      "4872/4872 [==============================] - 2s 347us/step - loss: 0.4649 - accuracy: 0.7888 - val_loss: 0.5681 - val_accuracy: 0.7521\n",
      "Epoch 14/50\n",
      "4872/4872 [==============================] - 2s 362us/step - loss: 0.4581 - accuracy: 0.7950 - val_loss: 0.5316 - val_accuracy: 0.7701\n",
      "Epoch 15/50\n",
      "4872/4872 [==============================] - 2s 348us/step - loss: 0.4488 - accuracy: 0.7982 - val_loss: 0.5753 - val_accuracy: 0.7406\n",
      "Epoch 16/50\n",
      "4872/4872 [==============================] - 2s 344us/step - loss: 0.4476 - accuracy: 0.7997 - val_loss: 0.5782 - val_accuracy: 0.7389\n",
      "Epoch 17/50\n",
      "4872/4872 [==============================] - 2s 350us/step - loss: 0.4428 - accuracy: 0.8001 - val_loss: 0.7046 - val_accuracy: 0.7176\n",
      "Epoch 18/50\n",
      "4872/4872 [==============================] - 2s 346us/step - loss: 0.4440 - accuracy: 0.8030 - val_loss: 0.5614 - val_accuracy: 0.7529\n",
      "Epoch 19/50\n",
      "4872/4872 [==============================] - 2s 345us/step - loss: 0.4380 - accuracy: 0.8042 - val_loss: 0.5924 - val_accuracy: 0.7578\n",
      "Epoch 20/50\n",
      "4872/4872 [==============================] - 2s 346us/step - loss: 0.4279 - accuracy: 0.8110 - val_loss: 0.5635 - val_accuracy: 0.7660\n",
      "Epoch 21/50\n",
      "4872/4872 [==============================] - 2s 351us/step - loss: 0.4234 - accuracy: 0.8167 - val_loss: 0.5827 - val_accuracy: 0.7644\n",
      "Epoch 22/50\n",
      "4872/4872 [==============================] - 2s 346us/step - loss: 0.4183 - accuracy: 0.8222 - val_loss: 0.7366 - val_accuracy: 0.7266\n",
      "Epoch 23/50\n",
      "4872/4872 [==============================] - 2s 345us/step - loss: 0.4191 - accuracy: 0.8192 - val_loss: 0.5546 - val_accuracy: 0.7365\n",
      "Epoch 24/50\n",
      "4872/4872 [==============================] - 2s 348us/step - loss: 0.4148 - accuracy: 0.8194 - val_loss: 0.5530 - val_accuracy: 0.7603\n",
      "Epoch 25/50\n",
      "4872/4872 [==============================] - 2s 348us/step - loss: 0.4066 - accuracy: 0.8220 - val_loss: 0.5043 - val_accuracy: 0.7701\n",
      "Epoch 26/50\n",
      "4872/4872 [==============================] - 2s 346us/step - loss: 0.4041 - accuracy: 0.8268 - val_loss: 0.5458 - val_accuracy: 0.7627\n",
      "Epoch 27/50\n",
      "4872/4872 [==============================] - 2s 348us/step - loss: 0.4027 - accuracy: 0.8272 - val_loss: 0.5686 - val_accuracy: 0.7619\n",
      "Epoch 28/50\n",
      "4872/4872 [==============================] - 2s 349us/step - loss: 0.3951 - accuracy: 0.8337 - val_loss: 0.6481 - val_accuracy: 0.7258\n",
      "Epoch 29/50\n",
      "4872/4872 [==============================] - 2s 356us/step - loss: 0.4013 - accuracy: 0.8266 - val_loss: 0.5452 - val_accuracy: 0.7718\n",
      "Epoch 30/50\n",
      "4872/4872 [==============================] - 2s 354us/step - loss: 0.3876 - accuracy: 0.8358 - val_loss: 0.6246 - val_accuracy: 0.7849\n",
      "Epoch 31/50\n",
      "4872/4872 [==============================] - 2s 345us/step - loss: 0.3895 - accuracy: 0.8378 - val_loss: 0.6279 - val_accuracy: 0.7176\n",
      "Epoch 32/50\n",
      "4872/4872 [==============================] - 2s 350us/step - loss: 0.3861 - accuracy: 0.8387 - val_loss: 0.6371 - val_accuracy: 0.7192\n",
      "Epoch 33/50\n",
      "4872/4872 [==============================] - 2s 348us/step - loss: 0.3805 - accuracy: 0.8387 - val_loss: 0.5396 - val_accuracy: 0.7759\n",
      "Epoch 34/50\n",
      "4872/4872 [==============================] - 2s 353us/step - loss: 0.3730 - accuracy: 0.8422 - val_loss: 0.5855 - val_accuracy: 0.7562\n",
      "Epoch 35/50\n",
      "4872/4872 [==============================] - 2s 355us/step - loss: 0.3677 - accuracy: 0.8454 - val_loss: 0.6975 - val_accuracy: 0.7537\n",
      "Epoch 36/50\n",
      "4872/4872 [==============================] - 2s 352us/step - loss: 0.3700 - accuracy: 0.8483 - val_loss: 0.6019 - val_accuracy: 0.7800\n",
      "Epoch 37/50\n",
      "4872/4872 [==============================] - 2s 355us/step - loss: 0.3635 - accuracy: 0.8506 - val_loss: 0.5248 - val_accuracy: 0.7734\n",
      "Epoch 38/50\n",
      "4872/4872 [==============================] - 2s 350us/step - loss: 0.3630 - accuracy: 0.8475 - val_loss: 0.5928 - val_accuracy: 0.7685\n",
      "Epoch 39/50\n",
      "4872/4872 [==============================] - 2s 352us/step - loss: 0.3567 - accuracy: 0.8491 - val_loss: 0.8017 - val_accuracy: 0.7586\n",
      "Epoch 40/50\n",
      "4872/4872 [==============================] - 2s 344us/step - loss: 0.3597 - accuracy: 0.8565 - val_loss: 0.5230 - val_accuracy: 0.7849\n",
      "Epoch 41/50\n",
      "4872/4872 [==============================] - 2s 355us/step - loss: 0.3504 - accuracy: 0.8532 - val_loss: 0.6147 - val_accuracy: 0.7824\n",
      "Epoch 42/50\n",
      "4872/4872 [==============================] - 2s 351us/step - loss: 0.3360 - accuracy: 0.8623 - val_loss: 0.5375 - val_accuracy: 0.7578\n",
      "Epoch 43/50\n",
      "4872/4872 [==============================] - 2s 352us/step - loss: 0.3298 - accuracy: 0.8690 - val_loss: 0.5608 - val_accuracy: 0.7389\n",
      "Epoch 44/50\n",
      "4872/4872 [==============================] - 2s 351us/step - loss: 0.3380 - accuracy: 0.8615 - val_loss: 0.4992 - val_accuracy: 0.7750\n",
      "Epoch 45/50\n",
      "4872/4872 [==============================] - 2s 355us/step - loss: 0.3195 - accuracy: 0.8688 - val_loss: 0.6424 - val_accuracy: 0.7627\n",
      "Epoch 46/50\n",
      "4872/4872 [==============================] - 2s 357us/step - loss: 0.3148 - accuracy: 0.8758 - val_loss: 0.7406 - val_accuracy: 0.7750\n",
      "Epoch 47/50\n",
      "4872/4872 [==============================] - 2s 354us/step - loss: 0.3290 - accuracy: 0.8676 - val_loss: 0.5375 - val_accuracy: 0.7594\n",
      "Epoch 48/50\n",
      "4872/4872 [==============================] - 2s 352us/step - loss: 0.3066 - accuracy: 0.8834 - val_loss: 0.6187 - val_accuracy: 0.7824\n",
      "Epoch 49/50\n",
      "4872/4872 [==============================] - 2s 354us/step - loss: 0.3021 - accuracy: 0.8810 - val_loss: 0.7263 - val_accuracy: 0.7742\n",
      "Epoch 50/50\n",
      "4872/4872 [==============================] - 2s 355us/step - loss: 0.3172 - accuracy: 0.8727 - val_loss: 0.6079 - val_accuracy: 0.7890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f12b4672a50>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "lstm_model.fit(X_train,\n",
    "               y_train,\n",
    "               batch_size=256,\n",
    "               epochs=50,\n",
    "               validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see how we did, apply binary mask over our sigmoid output\n",
    "preds = [1 if x >= 0.5 else 0 for x in lstm_model.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7806959947472094"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# and measure binary accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our model is scoring significantly better than random chance on the test set, we're not in the realm of certaintly that we might have liked. We may still be running into the trouble of too small sequences to form predictions on in our tweet space. We'll have to compare against our traditional methods using TF-IDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
